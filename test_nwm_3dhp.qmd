---
title: "NWM on 3DHP testing"
author: "dan.auerbach@dfw.wa.gov"
date: "`r Sys.Date()`"
format:
  html:
    embed-resources: true
    theme: yeti 
    code-fold: true
    toc: true
    toc-location: left
    grid:
      sidebar-width: 180px
      body-width: 1100px
      margin-width: 20px
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 8)

library("tidyverse", quietly = T)
library("sf")
library("patchwork")
library("gt")
theme_set(theme_minimal()) 

dir_data_common <- "~/T/DFW-Team WDFW Watershed Synthesis - data_common"
epsg <- 2927 #WA state standard; NAD83(HARN)/ft

load("test_nwm_3dhp.RData")
```

# objects

```{r sf_fpb, eval=FALSE}
sf_fpb <- sf::read_sf(
  file.path(dir_data_common,"WdfwFishPassage.gdb"), 
  layer = "WDFW_FishPassageSite") |> 
  sf::st_transform(epsg) |> 
  rename_with(.cols = starts_with("FishPassage"), .fn = ~str_replace(., "FishPassage","FP")) |> 
  filter(
    FPFeatureTypeCode==1 #culverts
    # #!is.na(BarrierCorrectionTypeCode)
    # !is.na(BarrierCorrectionYearsText)
  ) |> 
  mutate(
    fish_use = case_when(
      FishUseCode == 10 ~ "yes",
      FishUseCode == 20 ~ "no",
      FishUseCode == 99 ~ "unk"
    )
  ) |> 
  left_join(
    tibble(
      OwnerTypeCode = c(1:7,9,10,12), 
      owner = c("city", "county", "federal", "private", "state", "tribal", "other", "drainage_dist", "diking_dist", "unk")
    ),
    by = "OwnerTypeCode"
  )
```

```{r sf_nhdp_wa, eval=FALSE}
# #med res NHDplus, 230K polys including non-WA, no attrib but FEATUREID==COMID for StreamCat join
# sf_nhdp <- sf::read_sf(file.path(dir_data_common, "NHDPlus17/NHDPlusCatchment/Catchment.shp")) |>
#   sf::st_transform(crs = sf::st_crs(epsg)) |>
#   select(COMID = FEATUREID, areasqkm = AreaSqKM)
# 
# #use StreamCat for WA to nonspatially subset catchment polys
# sf_nhdp_wa <- inner_join(
#   sf_nhdp |> mutate(comid = as.character(COMID))
#   ,
#   StreamCatTools::sc_nlcd(state = "WA", year = "2019", aoi = 'riparian_watershed') |>
#     rename_with(tolower) |> as_tibble() |>
#     mutate(comid = as.character(comid)) |> 
#     select(comid, state)
#   ,
#   by = "comid")
# 
# saveRDS(sf_nhdp_wa, "~/T/DFW-Team WDFW Watershed Synthesis - flow_trees_heat/sf_nhdp_wa.rds")

sf_nhdp_wa <- readRDS("~/T/DFW-Team WDFW Watershed Synthesis - flow_trees_heat/sf_nhdp_wa.rds")
```

```{r lines_3dhp, eval=FALSE}
# sf::st_layers(file.path(dir_data_common, "3dhp/17110008_EDH_230605.gdb"))

# #after initial read before select()
# as_tibble(lines) |> #glimpse()
#   #count(FClass, EClass) #Fclass all 1, Eclass 0,2,3
#   #count(Source) #all "Lidar YYYY", mostly 2017 and 2013
#   #count(Method) #all "Flow direction and accumulation combined with proprietary methods"
#   #count(UserCode) #all NA except 2 " "
#   #count(Comments) #mostly NA, sort of interesting but probably not standard, in USGS?
#   count(GNIS_Name) #100 names and a few versions of blanks and nulls

# #test if multilinestring is actually more than single line
# #not, anywhere in the full set actually
# any(map_int(st_geometry(lines_3dhp), length) > 1)

#in 2927, 74K, note XYZ-ness
lines_3dhp <- sf::st_read(file.path(dir_data_common, "3dhp/17110008_EDH_230605.gdb"), layer = "Lines") |> 
  select(-c(FClass, Method, UserCode, Comments)) |> 
  sf::st_cast("LINESTRING") 

# lines_3dhp |> 
#   #as_tibble() |> glimpse()
#   ggplot() + geom_sf(aes(color = Source))

# #not clear exactly where this is...
# cat_3dhp <- sf::st_read(file.path(dir_data_common, "3dhp/1711000802_catchments/1711000802_catchments.shp"))

```

```{r add_slope_est}
lines_3dhp <- lines_3dhp |> 
  mutate(
    across(SHAPE, list(
      zslope_est = ~slopes::slope_xyz(., directed = T),
      zmax = ~slopes::z_max(.)
      ), .names = "{.fn}")
  )

# lines_3dhp |> 
#   st_centroid() |> 
#   ggplot() + geom_sf(aes(color = zslope_est), size = 0.5, alpha = 0.5) + wacolors::scale_color_wa_c("forest_fire")

```

cut nhdp to 3dhp & associate comid

```{r sf_nhdp_wa_3dhp, eval=FALSE}
sf_nhdp_wa_3dhp <- sf_nhdp_wa[st_as_sfc(st_bbox(lines_3dhp)),] #1464 of 57313

#not bad, maybe ~1min?
lines_3dhp <- st_join(
  lines_3dhp,
  sf_nhdp_wa_3dhp |> select(-state),
  join = st_intersects, #st_crosses?
  largest = T
)

```

cut 3dhp to focal comid(s) & build test network

```{r lines_3dhp_to_lsfn, eval=FALSE}
# #where is the greatest density of 3dhp lines in NHDplus MR catchments?
# sf_nhdp_wa_3dhp |> 
#   left_join(
#     lines_3dhp |> as_tibble() |> count(comid, name = "n_3dhp") |> arrange(desc(n_3dhp))    
#     , by = "comid"
#   ) |> 
#   drop_na(n_3dhp) |> 
#   ggplot() + geom_sf(aes(color = n_3dhp, fill = n_3dhp)) + 
#   wacolors::scale_color_wa_c("sea", reverse = T, aesthetics = c("color","fill")) + 
#   labs(title = "Pilot 3DHP lines per med res NHDplus catchments")
# 
# #for now just take the comid with the greatest number of associated 3dhp line units: "24275471"
# lines_3dhp |> as_tibble() |> count(comid, name = "n_3dhp") |> arrange(desc(n_3dhp)) |> slice(1:10)


# #spatial bounds initially generate a 'rooted forest' with GIS artifact strays
# ggplot() +
#   geom_sf(data = st_as_sf(lsfn, "edges"), col = "lightblue", linewidth = 0.3) +
#   geom_sf(
#     data = lsfn |> 
#       tidygraph::activate(nodes) |> 
#       filter(tidygraph::node_is_root()) |> 
#       st_as_sf("nodes")
#     ,
#     color = "orange"
#   )


#create sfnetwork object with nodes and edges that have from/to attributes added...
#identical feature count, follows from length(geometry()) test
#the convert to_largest_component drops GIS artifact strays from initial 'rooted forest'
#then adds calc'd topological and length weighted distance for nodes which are then associated to 'to' of edges
#map inspection that the ground distance versions are 'from downstream node to downstream [root] node'
lsfn <- lines_3dhp |> 
  filter(comid == "24275471") |> 
  select(uid = UniqueID, shape_length = SHAPE_Length, comid, desc = Desc) |> 
  sfnetworks::as_sfnetwork() |> 
  tidygraph::activate(nodes) |> 
  tidygraph::convert(
    tidygraph::to_largest_component,
    .clean = T #this drops the .tidygraph_node_index/edge_index that no longer apply & are potentially confusing
  ) |> 
  mutate(
    idn = seq_along(SHAPE),
    #raindrop perspective paths from nodes to root
    d_root = tidygraph::node_distance_to(tidygraph::node_is_root()),
    d_root_ft = tidygraph::node_distance_to(tidygraph::node_is_root(), weights = shape_length),
    d_root_km = 0.0003 * d_root_ft,
    #looking 'up' perspective to perimeter from per-node 'me' 
    in_n = tidygraph::local_size(order = nrow(tidygraph::.N()), mode = "in")
    ) 


#associate the immediately upstream streamlength from edges to each 'to' node
#summed over both upstream tribs
#also add 'below_culvert' logical:
#   every culvert edge has a single 'to' node,
#   but each of those 'to' nodes has potentially 2 incoming edges
#   one of which may be a non-culvert
#   and there is a potential case of 2 converging culverts
#   so need logic to define per-node state in 
#   {no-incoming, not, culvert, not+not, culvert+not, culvert+culvert}
lsfn <- lsfn |> 
  tidygraph::activate(nodes) |> 
  left_join(
    select(as_tibble(as_tibble(lsfn, active = "edges")), uid, idn = to, shape_length, desc) |> 
      summarise(
        below_culvert = if_else(any(str_detect(desc, "ulvert")), T, F),
        in_ft_first_order = sum(shape_length),
        .by = "idn"),
    by = c("idn")
    )

#ensure whole thing is directed acyclic to root
tidygraph::with_graph(lsfn, tidygraph::graph_is_dag()) #true, good

#now per-node, define upstream neighborhood and sum over first-order incoming lengths of neighborhood members
#sweet: https://tidygraph.data-imaginist.com/reference/map_local.html
#this may not be the most efficient approach but...
#no appreciable speed diff between order = 3,10,max
lsfn_order <- tidygraph::with_graph(lsfn, tidygraph::graph_order())

lsfn <- lsfn |> 
  tidygraph::activate(nodes) |> 
  mutate(
    in_ft = tidygraph::map_local_dbl(
      order = lsfn_order, mode = "in", 
      .f = function(neighborhood, ...){
        sum(as_tibble(neighborhood, active = 'nodes')$in_ft_first_order, na.rm = T)
        }
      ),
    in_km = 0.0003 * in_ft
  )


#and then add node attributes back to edge by 'to'
lsfn <- lsfn |> 
  tidygraph::activate(edges) |> 
  mutate(culvert = str_detect(desc, "ulvert")) |> 
  left_join(
    select(as_tibble(as_tibble(lsfn, active = "nodes")), to = idn, d_root, ends_with("_km")) 
    ,
    by = c("to")
    ) |> 
  tidygraph::activate(nodes)
  


## unused but keeping handy for now
## temporarily converting edges to nodes 
# lsfn |> 
#   tidygraph::morph(
#     tidygraph::to_linegraph
#   ) |> 
#   mutate(
#     d_root = tidygraph::node_distance_to(tidygraph::node_is_root())
#   ) |> 
#   tidygraph::unmorph() 

```

```{r func_prep_sfn}
#little wrapper to pull out 3dhp for a given catchment
#then make into attributed sfnetwork
#follow above flow in chunk lines_3dhp_to_lsfn
prep_sfn <- function(cat){
  l <- lines_3dhp |> 
    filter(comid == cat) |> 
    select(uid = UniqueID, shape_length = SHAPE_Length, comid, desc = Desc, zslope_est, zmax) |> 
    sfnetworks::as_sfnetwork() |>
    tidygraph::activate(nodes) |>
    tidygraph::convert(
      tidygraph::to_largest_component,
      .clean = T #this drops the .tidygraph_node_index/edge_index that no longer apply & are potentially confusing
    ) |>
    mutate(
      idn = seq_along(SHAPE),
      #raindrop perspective paths from nodes to root
      d_root = tidygraph::node_distance_to(tidygraph::node_is_root()),
      d_root_ft = tidygraph::node_distance_to(tidygraph::node_is_root(), weights = shape_length),
      d_root_km = 0.0003 * d_root_ft,
      #looking 'up' perspective to perimeter from per-node 'me'
      in_n = tidygraph::local_size(order = nrow(tidygraph::.N()), mode = "in")
    )

  #ensure whole thing is directed acyclic to root
  if(!tidygraph::with_graph(l, tidygraph::graph_is_dag())) {stop();print("Not DAG")}

  #associate the immediately upstream streamlength from edges to each 'to' node
  #summed over both upstream tribs; also add 'below_culvert' logical:
  # defining per-node state in {no-incoming, not, culvert, not+not, culvert+not, culvert+culvert}
  l <- l |>
    tidygraph::activate(nodes) |>
    left_join(
      select(as_tibble(as_tibble(l, active = "edges")), uid, idn = to, shape_length, desc, zslope_est, zmax) |>
        summarise(
          below_culvert = if_else(any(str_detect(desc, "ulvert")), T, F),
          in_slope_mean = mean(zslope_est),
          in_ft_first_order = sum(shape_length),
          .by = "idn"),
      by = c("idn")
    )

  #now per-node, define upstream neighborhood and sum over first-order incoming lengths of neighborhood members
  l_order <- tidygraph::with_graph(l, tidygraph::graph_order())
  
  l <- l |>
    tidygraph::activate(nodes) |>
    mutate(
      in_ft = tidygraph::map_local_dbl(
        order = l_order, mode = "in",
        .f = function(neighborhood, ...){
          sum(as_tibble(neighborhood, active = 'nodes')$in_ft_first_order, na.rm = T)
        }
      ),
      in_km = 0.0003 * in_ft
    )

  #and then add node attributes back to edge by 'to'
  l <- l |>
    tidygraph::activate(edges) |>
    mutate(culvert = str_detect(desc, "ulvert")) |>
    left_join(
      select(as_tibble(as_tibble(l, active = "nodes")), to = idn, d_root, ends_with("_km"))
      ,
      by = c("to")
    ) |>
    tidygraph::activate(nodes)
  
  return(l)
}


lsfn <- prep_sfn("24275471")
```


# perfuse fixed root value

## alt1: topology + channel distance

playing with reverse "contributing" + "travel time" to get a cfs-unit value 'up'

the "in_km" built from topology and edge geometry length is proxy, not true 'drainage area', especially over very large domains or other (lithology, land cover) factors that cause adjoining subbasins to have very different drainage density/dissection.

Need to progress iteratively from network perimeter down, at a multi-COMID scale?
This first pass going to create big flow 'jumps' where mainstem catchments join?


```{r dumb_instant_travel_perfuse}

z <- as_tibble(lsfn, active = 'nodes')

#z |> sf::st_zm() |> select(-in_node) |> mapview::mapview(zcol = "in_ft")

#hot damn
ggplot() +
  geom_sf(data = as_tibble(lsfn, active = "edges"), color = "lightblue", linewidth = 0.5) +
  geom_sf(data = z, aes(color = log10(in_ft)), alpha = 0.8) +
  wacolors::scale_color_wa_b()

#try for super dumb demo of 'spreading a fixed volume up network'?
#good: decay is appropriately at power-law rate, 
# so is this capturing nested/recursive/cumulative contribution more or less correctly?
#bad/weird: this is showing a weird thing, unclear what it means in flow units
# "if the flow at the outlet is 1000, then this is the independent portion contributed by each network position" 
z |>
  #as_tibble() |> 
  arrange(desc(in_km)) |> 
  mutate(
    in_km_pct = in_km / sum(in_km),
    q1k =  1000*in_km_pct,
    q1k_log10 = log10(q1k)
  ) |> 
  #arrange(q1k) |> filter(!is.infinite(q1k_log10))
  #pull(q1k) |> sum(na.rm = T)
  ggplot() + 
  geom_sf(data = st_as_sf(lsfn, "edges"), col = "lightblue", linewidth = 0.3) +
  geom_sf(aes(color = q1k_log10)) + wacolors::scale_color_wa_b("ferries", reverse = T)


#check summing upstream in subgraph
#this is somewhat reassuring: the 'outlet' flow is scaling appropriately within-network
lsfn <- lsfn |> 
  tidygraph::activate("nodes") |> 
  mutate(
    in_km_pct = in_km / sum(in_km),
    q1k =  1000*in_km_pct,
    q1k_log10 = log10(q1k)
  )

z <- lsfn |> 
  tidygraph::activate("nodes") |> 
  mutate(
    q_recalc = tidygraph::map_local_dbl(
      order = lsfn_order, mode = "in", 
      .f = function(neighborhood, ...){
        sum(as_tibble(neighborhood, active = 'nodes')$q1k, na.rm = T)
        }
      )
  )


{
as_tibble(z, active = 'nodes') |> #pull(q_recalc) |> summary()
  ggplot() + 
  geom_sf(data = st_as_sf(lsfn, "edges"), col = "lightblue", linewidth = 0.3) +
  geom_sf(aes(color = q_recalc), size = 0.5) + 
  wacolors::scale_color_wa_c("ferries", reverse = T) +
  labs(
    title = "Assuming instantaneous travel time, \n
       if outlet flow is 1000 cfs/cms/etc, and independent network contributions are uniform, \n
       then this is the (instant) value everywhere in the network, \n
       with Q per node summed from upstream subgraph (in-neighborhood)
    ")
}+{
as_tibble(z, active = 'nodes') |> 
  ggplot() + 
  geom_sf(data = st_as_sf(lsfn, "edges"), col = "lightblue", linewidth = 0.3) +
  geom_sf(aes(color = log10(q_recalc)), size = 0.5) + 
  wacolors::scale_color_wa_b("ferries", n.breaks = 6, reverse = T) +
  labs(title = "same, log10 to show power law decay")
} + plot_layout(ncol = 1)

ggsave(filename = "figures/f_ins_tt_unif_contrib_1k.png", width = 7, height = 11, dpi = 150, bg = "white")

```

## alt2a: alt1 + gradient

loop back to, but paused pending further review of [Brinkerhoff et al.](https://github.com/craigbrinkerhoff/CONUS_ephemeral)

# what about desc==culvert (& vs fpb)

> "The elevation derived hydrography specification calls for the segmentation and distinction of culvert features from the rest of the flowline network. Culvert features were automatically identified by comparing the monotonically enforced elevation values to the elevation ground model. Vertices that are misaligned with the DEM surface after monotonic Z smoothing and in close proximity to ancillary road and culvert data were extracted and used to classify culverts within the network. This segmentation process was manually reviewed. Additional culverts were manually extracted as necessary." -- NV5 Geospatial, pilot project report "Stillaguamish Watershed Lidar-derived 3DHP", June 9, 2023

"Connector: culvert" reaches (edges) in terms of the upstream network km

```{r func_gg_culv}
#conn_culvert_upstream

# as_tibble(lsfn, "edges") |> 
#   #as_tibble() |> count(desc)
#   #ggplot() + geom_sf(aes(color = desc)) + scale_color_manual(values = c("grey","orange","lightblue"))
#   ggplot() + geom_sf(aes(color = culvert)) + scale_color_manual(values = c("lightblue","orange"))
# 
# as_tibble(lsfn, active = "nodes") |> ggplot() + geom_sf(aes(color = below_culvert))
# 
# as_tibble(lsfn, "edges") |> st_zm() |> mapview::mapview(zcol = "culvert", burst = T)

# ggplot() +
#   geom_sf(data = as_tibble(lsfn, "edges") |> filter(!culvert), linewidth = 0.2, color = "lightblue") +
#   geom_sf(data = as_tibble(lsfn, "nodes") |> filter(below_culvert), aes(color = in_km, size = in_km)) + 
#   scale_color_gradient(low = "white", high = "purple") + scale_size_area(max_size = 4) +
#   theme(line = element_blank(), axis.text = element_blank()) +
#   ggspatial::annotation_scale(location = "bl")

gg_culv <- function(){
  ggplot() +
    geom_sf(data = as_tibble(lsfn, "edges") |> filter(!culvert), linewidth = 0.2, color = "lightblue") +
    geom_sf(data = as_tibble(lsfn, "nodes") |> filter(below_culvert),
            #aes(color = in_km, size = 1 / d_root_km)) + 
            aes(size = in_km, color = 1 / d_root_km)) + 
    #scale_color_gradient(low = "tan", high = "purple") + 
    wacolors::scale_color_wa_b("puget", n.breaks = 8, reverse = T) + 
    scale_size_area(max_size = 5) +
    theme(line = element_blank(), axis.text = element_blank()) +
    ggspatial::annotation_scale(location = "bl") +
    labs(subtitle = paste("3DHP for NHD comid", as_tibble(lsfn, "edges")$comid[1]))
}

#gg_culv()

# lsfn |>
#   mutate(
#     ac = tidygraph::centrality_alpha()
#   ) |> filter(in_n != ac)

```

```{r sf_nhdp_wa_3dhp_fpb}
#slightly tighter than bbox
lines_3dhp_uch <- lines_3dhp |> st_union() |> st_convex_hull()

#cut FPDSI subset down to 3dhp
#sf_fpb_3dhp <- sf_fpb[st_as_sfc(st_bbox(lines_3dhp)),] #this is 2221
sf_fpb_3dhp <- sf_fpb[lines_3dhp_uch,] #1374

#as_tibble(sf_fpb_3dhp) |> count(fish_use)

sf_fpb_3dhp |> ggplot() + geom_sf(aes(shape = fish_use, color = fish_use)) + geom_sf(data = lines_3dhp_uch, fill = NA, color = "hotpink")

#add a basic barrier attribute count to NHDcats
sf_nhdp_wa_3dhp_fpb <- sf_nhdp_wa_3dhp |> 
  select(-state) |> 
  left_join(
    st_join(sf_fpb_3dhp, sf_nhdp_wa_3dhp) |> #pt in poly, not expanding
      as_tibble() |> 
      count(comid, fish_use) |> 
      pivot_wider(names_from = fish_use, values_from = n)
    ,
    by = "comid"
  )


{ggplot() + geom_sf(data = sf_nhdp_wa_3dhp_fpb, aes(fill = yes), color = NA) + 
    geom_sf(data = lines_3dhp_uch, fill = NA, color = "hotpink") +
    wacolors::scale_fill_wa_c("forest_fire", na.value = "grey90")
  }+{ggplot() + geom_sf(data = sf_nhdp_wa_3dhp_fpb, aes(fill = no), color = NA) + 
      geom_sf(data = lines_3dhp_uch, fill = NA, color = "hotpink") +
      wacolors::scale_fill_wa_c("ferries", na.value = "grey90")} +
  plot_layout(ncol = 1) + 
  plot_annotation(subtitle = "Fish passage barriers per 1:100NHDplus catchment by 'fish use' \n pink hull shows 3DHP extent")

```

```{r culv_eda}
#first couple cats not in 3dhp, train on 24275405 32+5
sf_nhdp_wa_3dhp_fpb[lines_3dhp_uch,] |> arrange(desc(yes+no))

fcat <- "24275405"
lsfn <- prep_sfn(fcat)
#can should instead reverse join poly comid on point...
lsfn_fpb <- sf_fpb_3dhp[filter(sf_nhdp_wa_3dhp_fpb, comid == fcat),]

gg_culv() +
  geom_sf(
    data = lsfn_fpb, aes(shape = fish_use), 
    color = "grey30", alpha = 0.5) + scale_shape_manual(values = c(3,4))

mapview::mapview(st_zm(as_tibble(lsfn, "edges")), zcol = "culvert", burst = T, color = c("lightblue","hotpink")) +
  mapview::mapview(lsfn_fpb, zcol = "fish_use", burst = T)

#compare field assessed slope for a match: uid 65142
#https://apps.wdfw.wa.gov/fishpassagephotos/Reports/LP27_Report.pdf
#slope 1.03%


# #remotes::install_github("ropensci/slopes")
# #from slope_matrix workhorse help "The output value is a proportion representing the change in elevation for a given change in horizontal movement along the linestring. 0.02, for example, represents a low gradient of 2% while 0.08 represents a steep gradient of 8%."
# z <- as_tibble(lsfn, "edges") |> filter(uid == "65142")
# 
# slopes::z_value(z)
# slopes::slope_xyz(z, directed = T)
# #need to confirm that in reaches/edges 
# #first point is upstream/max
# #last point is downstream/min
# z |> mutate(
#   across(SHAPE, list(
#     zslope_est = ~slopes::slope_xyz(., directed = T),
#     z1 = ~slopes::z_start(.),
#     z2 = ~slopes::z_end(.),
#     #zmin = ~slopes::z_min(.), #bug, calls max(), issue created
#     zmin = ~min(slopes::z_value(.)), 
#     zmax = ~slopes::z_max(.)
#   ))
# ) |> select(uid, contains("z"))
# 
# #cannot get to scatter against FPDSI slope without different version of those data (not going to review individual level A reports)
# #but can still start to look at (mis)match...
# #inflections at ~20ft buffer distance 
# tibble(d = seq(0, 50, by = 2)) |> 
#   mutate(
#     n_intsct = d |> 
#       map_int(
#         #st_intersection(
#         ~st_intersects(
#           as_tibble(lsfn, "edges") |> filter(culvert),
#           lsfn_fpb |> st_buffer(dist = .x)
#         ) |> unlist() |> length()
#       )
#   ) |> 
#   ggplot() + geom_col(aes(d, n_intsct))
# 
# d_buff <- 20
# 
# #if just inner_join wanted can use st_intersection
# #but this preserves non-matches as NA
# z <- st_join(
#   as_tibble(lsfn, "edges") |> filter(culvert),
#   lsfn_fpb |> st_buffer(dist = d_buff)
#   ) |> 
#   mutate(
#     across(SHAPE, list(
#       zslope_est = ~slopes::slope_xyz(., directed = T),
#       zmax = ~slopes::z_max(.)
#       ), .names = "{.fn}")
#   )
# 
# z |> 
#   ggplot(aes(fish_use, zslope_est)) +
#   geom_jitter(width = 0.2, alpha = 0.6) +
#   geom_violin(fill = NA)

lines_3dhp_culv <- lines_3dhp |> 
    filter(str_detect(Desc, "ulvert")) 

#full set? (FAST!?!)
sf_3dhp_culv_fpb <- st_join(
  lines_3dhp_culv
  ,
  sf_fpb_3dhp |> st_buffer(dist = d_buff)
  ) |> 
  mutate(
    across(SHAPE, list(
      zslope_est = ~slopes::slope_xyz(., directed = T),
      zmax = ~slopes::z_max(.)
      ), .names = "{.fn}")
  ) |> 
  select(
    uid = UniqueID, shape_length = SHAPE_Length, comid, desc = Desc,
    SiteId, FeatureType, fish_use, FishUseCriteriaCode, FPBarrierStatusCode, FPBarrierReasonCode, PercentFishPassableCode, SurveyDate,
    zslope_est, zmax
    )

sf_3dhp_culv_fpb |> #summary()
  filter(zslope_est <= 0) |>  #drop a single edge with a positive slope (assume this is artifact)
  ggplot(aes(fish_use, zslope_est)) +
  geom_jitter(width = 0.2, alpha = 0.6) +
  geom_violin(fill = NA)

#plot coerced to centroid point for better vis
{
ggplot() +
  geom_sf(
    data = sf_3dhp_culv_fpb |>
      filter(is.na(SiteId), zslope_est <= 0) |> #nrow()
      st_centroid(),
    aes(color = zslope_est), shape = 18, alpha = 0.6
    ) + 
    wacolors::scale_color_wa_c("foothills") +
    labs(subtitle = "8,332 3DHP without (20ft buffer) matches to fish passage inventory culverts")
}+{
ggplot() +
  geom_sf(
    data = sf_3dhp_culv_fpb |>
      drop_na(SiteId) |> 
      filter(zslope_est <= 0) |> #nrow() #drop a single edge with a positive slope (assume this is artifact)
      st_centroid(),
    aes(color = zslope_est, shape = fish_use), alpha = 0.6
    ) +
  wacolors::scale_color_wa_c("foothills") +
  scale_shape_manual(values = c(19,10,15)) +
    labs(subtitle = "462 3DHP with (20ft buffer) matches")
} + {
  ggplot() +
    geom_sf(
      data = sf_fpb_3dhp |> 
        anti_join(as_tibble(sf_3dhp_culv_fpb) |> drop_na(SiteId), by = "SiteId")
      ,
      aes(shape = fish_use), alpha = 0.6
    ) +
    geom_sf(data = lines_3dhp_uch, fill = NA, color = "hotpink") +
    scale_shape_manual(values = c(19,10,15)) +
    labs(subtitle = "915 fish passage inventory culverts without 3DHP barrier matches \n (including those within convex hull but outside current 3DHP linework extent)")
} +
  plot_layout(ncol = 1) +
  plot_annotation(
    title = "Estimated slopes for 3DHP 'connector: culvert' reaches vs fish passage inventory culverts",
    subtitle = "Approx. ~5% of 3DHP 'connector: culvert' (462 of 8794) with barrier matches")

ggsave(filename = "figures/f_culv_slope_compare.png", width = 7, height = 11, dpi = 150, bg = "white")



#examine survey date/source for nonmatches/far distances
sf_fpb_3dhp |> 
  as_tibble() |> 
  #semi_join(as_tibble(sf_3dhp_culv_fpb), by = "SiteId") |> 
  anti_join(as_tibble(sf_3dhp_culv_fpb), by = "SiteId") |> 
  count(DataSource) |> arrange(desc(n))


#Euclidean distances from all fpb to any/culvert 3dhp lines
#takes a minute, returns n-barriers by n-reaches matrix
#need/want to integrate st_nearest_feature?
d_fpb_culv <- st_distance(
  sf_fpb_3dhp,  #barrier points
  lines_3dhp_culv, #culvert line sections only
)

#per barrier, along rows 
sf_fpb_3dhp$d_culv <- round(apply(d_fpb_culv, 1, min), 2)
sum(sf_fpb_3dhp$d_culv <= 20) #matches st_intersect on st_buffer(20) in st_join
plot.ecdf(sf_fpb_3dhp$d_culv)
#worst issues appear due to using convex_hull to constrain barrier set
sf_fpb_3dhp |> 
  mutate(d_in20 = d_culv <= 20) |> 
  #filter(d_culv < 1000) |> 
  ggplot() + geom_sf(aes(color = log10(d_culv))) + facet_wrap(~d_in20, ncol = 1)
#can also test against full linework set for barriers that are on/close to 'stream' reaches (missing in 3DHP?)

#distributions of distances to nearest culvert line by source
sf_fpb_3dhp |> 
  filter(d_culv < 1000) |> 
  ggplot() + 
  geom_boxplot(aes(d_culv, DataSource), varwidth = T)

#unique(nchar(sf_fpb_3dhp$SurveyDate))
sf_fpb_3dhp$date = as.Date(sf_fpb_3dhp$SurveyDate, format = "%m/%d/%Y")
#sf_fpb_3dhp |> filter(is.na(date)) |> select(SiteId, DataSource, SurveyDate)

sf_fpb_3dhp |> 
  ggplot() +
  geom_point(aes(year(date), d_culv, color = DataSource))


```

fit for 'prob of being a real culvert'?

split the basin, train on fpb matches, test/validate in remaining?

much more to think about doing with `LinealGainMeasurement`, `SignificantReachCode`, etc.

run version of climate adapted, given slope and some DA/contributing channel length regression on width?

# testing SSNbler & SSN2

```{r compare_ssnbler_vs_lsfn}

# SSNbler::copy_streams_to_temp()
# path <- paste0(tempdir(), "/streamsdata")
# MF_streams <- st_read(paste0(path, "/MF_streams.gpkg"))
# MF_obs <- st_read(paste0(path, "/MF_obs.gpkg"))
# MF_pred1km <- st_read(paste0(path, "/MF_pred1km.gpkg"))
# MF_CapeHorn <- st_read(paste0(path, "/MF_CapeHorn.gpkg"))
# MF_streams

path_ssn <- "~/T/DFW-Team WDFW Watershed Synthesis - General/hydrography/test_ssnbler"
ssn <- list()
#test with default lsfn from prep_sfn("24275471") of 1505 reaches
ssn$edges <-SSNbler::lines_to_lsn(
  streams = as_tibble(lsfn, "edges"),
  lsn_path = path_ssn,
  overwrite = T
)

ssn$edges <- SSNbler::updist_edges(
  ssn$edges,
  lsn_path = path_ssn,
  calc_length = T
)

ssn$edges #looks to match d_root_ft at least to reasonable precision

#vignette example uses presupplied drainage area "h2oAreaKm2"
#this may be a 3DHP attribute, but for now testing with per-node upstream in-length 
#ran quickly...
ssn$edges <- SSNbler::afv_edges(
  ssn$edges,
  lsn_path = path_ssn,
  infl_col = "in_km",
  segpi_col = "in_km_PI",
  afv_col = "in_km_AFV"
)

ssn$edges |> 
  ggplot() + 
  #geom_density(aes(x = in_km_AFV))
  geom_density(aes(x = in_km_PI))

#not yet run...should pick a test set of obs points
#appears to require arg 'ssn_path' that copies/creates new gpkg
#edges.gpkg written from above is 2.5mb; avoidable/aliasable for larger data?
SSNbler::ssn_assemble(
  edges = ssn$edges,
  lsn_path = path_ssn,
  
)

```


# Flow regime via NWM on 3DHP

come back to examine relative to: https://gis.dnr.wa.gov/site2/rest/services/Public_Forest_Practices/WADNR_PUBLIC_FP_Water_Type/MapServer

```{r nwm_zarr_pull3, eval=FALSE}
#revised from flow_trees_apps.qmd, which built per-year rds objects into data_common/nmw_retro, moved into /v2_1

# #https://github.com/grimbough/Rarr
# if (!require("BiocManager", quietly = TRUE))
#     install.packages("BiocManager")
# ## install Rarr
# BiocManager::install("Rarr")

# remotes::install_github(repo = "grimbough/Rarr")

library(tidyverse)
library(sf)
#for COMIDs all of WA
sf_nhdp_wa <- readRDS("~/T/DFW-Team WDFW Watershed Synthesis - flow_trees_heat/sf_nhdp_wa.rds")
sf_nhdp_wa_comid <- sf_nhdp_wa |> 
    as_tibble() |> 
    dplyr::filter(COMID > 0, areasqkm < 1500) |> 
    select(comid = COMID)
rm(sf_nhdp_wa)

library(Rarr)

#zarr_overview("https://noaa-nwm-retrospective-2-1-zarr-pds.s3.amazonaws.com/chrtout.zarr/streamflow/")
#structure changes slightly between 2.1 and 3.0
zarr_overview("https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/streamflow/")


# #may be worth truncating largest and smallest?
# as_tibble(sf_nhdp_wa) |> pull(areasqkm) |> summary()
# as_tibble(sf_nhdp_wa) |> filter(between(areasqkm, 0, 1000)) |>  ggplot() + stat_ecdf(aes(areasqkm))
# as_tibble(sf_nhdp_wa) |> arrange(desc(areasqkm))
# as_tibble(sf_nhdp_wa) |> arrange(areasqkm)
# as_tibble(sf_nhdp_wa) |> count(areasqkm < 0.001)
# #several very large, largest are cross border
# sf_nhdp_wa |> filter(areasqkm > 1500) |> select(-comid) |> mapview::mapview(zcol = "areasqkm")
# #various very small, scattered throughout
# sf_nhdp_wa |> filter(areasqkm < 0.001) |> select(-comid) |> sf::st_centroid() |> mapview::mapview(zcol = "areasqkm")

# #unclear what is happening with the negative COMIDs
# #somewhat randomly in the center of the state, some have NLCD riparian attrs
# #no negatives in NWM zarr feature_id
# as_tibble(sf_nhdp_wa) |> count(COMID < 0)
# sf_nhdp_wa |> filter(COMID < 0) |> select(-comid) |> mapview::mapview(zcol = "areasqkm")
# sf_nhdp_wa_nlcd19 |> filter(COMID < 0) |> as_tibble() |> print(n=100)


#zarr 'feature_id' *values* are COMIDs, but index position in feature_id != COMID 
#this reads the vector of all available NHD+ ids
#then subsets to those in the WA catchment polygons (excluding 5 in Canada and those with negative COMIDs)
#manually finding the correct url by hovering on html view; presumably a better way
#https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/index.html#CONUS/zarr/chrtout.zarr/feature_id/

# #submitted issue:  https://github.com/grimbough/Rarr/issues/10
# #old, worked, seems likely would now fail due to string rather than list datatype
# array_path <- "https://noaa-nwm-retrospective-2-1-zarr-pds.s3.amazonaws.com/chrtout.zarr/feature_id/"
# Rarr:::read_array_metadata(array_path)
# #new, fails
# array_path <- "https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/feature_id"  
# metadata <- Rarr:::read_array_metadata(array_path)
# #decompressor <- metadata$compressor$id # decompressor == "zstd", previously/above 'lz4'
# #fails: NULL, key is '$dtype', value is '<i8' which breaks get_chunk_size()
# datatype <- metadata$datatype
# #still wrong, returns string "<i8"
# datatype <- metadata$dtype 
# #should be list from
# datatype <- Rarr:::.parse_datatype(metadata$dtype)
# #which allows declaration
# buffer_size <- Rarr:::get_chunk_size(datatype, dimensions = metadata$chunks)

## horrible debugging
# #stepping through read_chunk
# i = 1
# chunk_id = required_chunks[i, ]
# dim_separator <- ifelse(is.null(metadata$dimension_separator),
#     yes = ".", no = metadata$dimension_separator
#   )
# chunk_id <- paste0(chunk_id, collapse = dim_separator)
# #chunk_file <- paste0(zarr_array_path, chunk_id) #looks like a bad path, maybe part of problem?
# chunk_file <- paste0(zarr_array_path,"/", chunk_id) #Yes
# #in the 'else' of isnull(s3)
# parsed_url <- Rarr:::parse_s3_path(chunk_file)
# #fails/returns NULL without coercing in the '/', fetches when included
# if(Rarr:::.s3_object_exists(s3_client, parsed_url$bucket, parsed_url$object)) {
#   compressed_chunk <- s3_client$get_object(Bucket = parsed_url$bucket,
#                                            Key = parsed_url$object)$Body
# }
# decompressed_chunk <- Rarr:::.decompress_chunk(compressed_chunk, metadata)
# #this is where the "e-320" appears
# converted_chunk <- Rarr:::.format_chunk(decompressed_chunk, metadata, alt_chunk_dim)
# 
# alt_chunk_dim <- unlist(metadata$chunks) #declared in extract_elements and passed
# datatype <- metadata$datatype
# actual_chunk_size <- length(decompressed_chunk) / datatype$nbytes
# if((datatype$base_type == "py_object") || (actual_chunk_size == prod(unlist(metadata$chunks)))) {
#     chunk_dim <- unlist(metadata$chunks)
#   } else if (actual_chunk_size == prod(alt_chunk_dim)) {
#     chunk_dim <- alt_chunk_dim
#   }
# output_type <- switch(datatype$base_type,
#                       "boolean" = 0L,
#                       "int" = 1L,
#                       "uint" = 1L,
#                       "float" = 2L
# )
# converted_chunk <- .Call("type_convert_chunk", decompressed_chunk,
#                              1L, #output_type, 
#                              datatype$nbytes, 
#                              datatype$is_signed,
#                              chunk_dim,
#                              PACKAGE = "Rarr"
#     )

# #trying to actually get the data, can prob due featureID part manually as fallback but not workable for flows
# #stepping throug read_zarr_array
# zarr_array_path <- "https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/feature_id"  
# s3_client <- Rarr:::.create_s3_client(path = zarr_array_path)
# metadata <- Rarr:::read_array_metadata(zarr_array_path)
# metadata$datatype
# metadata$datatype$base_type <- "float" #corrects buffer_size but not missing slash
# index <- vector(mode = "list", length = length(metadata$shape))
# index <- Rarr:::check_index(index = index, metadata = metadata)
# required_chunks <- as.matrix(Rarr:::find_chunks_needed(metadata, index))
# 
# #now step down into read_data
# #so deconstruct Rarr:::read_data
# chunk_idx <- mapply(\(x,y) { (x-1) %/% y }, index, metadata$chunks, SIMPLIFY = FALSE)
# #now step down into chunk_selections declaration
# #which means into .extract_elements and then into read_chunk and .format_chunk


my_format_chunk <- function (decompressed_chunk, metadata, alt_chunk_dim) 
{
    datatype <- metadata$datatype
    actual_chunk_size <- length(decompressed_chunk)/datatype$nbytes
    if ((datatype$base_type == "py_object") || (actual_chunk_size == 
        prod(unlist(metadata$chunks)))) {
        chunk_dim <- unlist(metadata$chunks)
    }
    else if (actual_chunk_size == prod(alt_chunk_dim)) {
        chunk_dim <- alt_chunk_dim
    }
    else {
        stop("Decompresed data doesn't match expected chunk size.")
    }
    if (metadata$order == "C") {
        chunk_dim <- rev(chunk_dim)
    }
    if (datatype$base_type == "string") {
        converted_chunk <- .format_string(decompressed_chunk, 
            datatype)
        dim(converted_chunk[[1]]) <- chunk_dim
    }
    else if (datatype$base_type == "unicode") {
        converted_chunk <- .format_unicode(decompressed_chunk, 
            datatype)
        dim(converted_chunk[[1]]) <- chunk_dim
    }
    else if (datatype$base_type == "py_object") {
        converted_chunk <- .format_object(decompressed_chunk, 
            metadata, datatype)
        dim(converted_chunk[[1]]) <- chunk_dim
    }
    else {
        output_type <- switch(datatype$base_type, boolean = 0L, 
#            int = 1L, uint = 1L, float = 2L)
            int = 1L, uint = 1L, float = 1L) #force float to 1L, not a general fix
        converted_chunk <- .Call("type_convert_chunk", decompressed_chunk, 
            output_type, datatype$nbytes, datatype$is_signed, 
           chunk_dim, PACKAGE = "Rarr")
    }
    if (metadata$order == "C") {
        converted_chunk[[1]] <- aperm(converted_chunk[[1]])
    }
    names(converted_chunk) <- c("chunk_data", "warning")
    return(converted_chunk)
}

#changes: add Rarr::: throughout; update chunk_file declaration to include '/' 
my_read_chunk <- function (zarr_array_path, chunk_id, metadata, s3_client = NULL, 
    alt_chunk_dim = NULL) 
{
    if (missing(metadata)) {
        metadata <- Rarr:::read_array_metadata(zarr_array_path, s3_client = s3_client)
    }
    dim_separator <- ifelse(is.null(metadata$dimension_separator), 
        yes = ".", no = metadata$dimension_separator)
    chunk_id <- paste(chunk_id, collapse = dim_separator)
    datatype <- metadata$datatype
#    chunk_file <- paste0(zarr_array_path, chunk_id)
    chunk_file <- paste0(zarr_array_path,"/", chunk_id)

    if (nzchar(Sys.getenv("RARR_DEBUG"))) {
        message(chunk_file)
    }
    if (is.null(s3_client)) {
        size <- file.size(chunk_file)
        if (file.exists(chunk_file)) {
            compressed_chunk <- readBin(con = chunk_file, what = "raw", 
                n = size)
        }
        else {
            compressed_chunk <- NULL
        }
    }
    else {
        parsed_url <- Rarr:::parse_s3_path(chunk_file)
        if (Rarr:::.s3_object_exists(s3_client, parsed_url$bucket, parsed_url$object)) {
            compressed_chunk <- s3_client$get_object(Bucket = parsed_url$bucket, 
                Key = parsed_url$object)$Body
        }
        else {
            compressed_chunk <- NULL
        }
    }
    if (!is.null(compressed_chunk)) {
        decompressed_chunk <- Rarr:::.decompress_chunk(compressed_chunk, 
            metadata)
#        converted_chunk <- Rarr:::.format_chunk(decompressed_chunk, 
        converted_chunk <- my_format_chunk(decompressed_chunk, 
            metadata, alt_chunk_dim)
    }
    else {
        converted_chunk <- list(chunk_data = array(metadata$fill_value, 
            dim = unlist(metadata$chunks)), warning = 0L)
    }
    return(converted_chunk)
}

#need to declare here as not even ':::' available
#also changing to:   chunk <- my_read_chunk(
my_extract_elements <- function(i, metadata, index, required_chunks, zarr_array_path, s3_client, chunk_idx) {
  ## find elements to select from the chunk and what in the output we replace
  index_in_result <- index_in_chunk <- list()
  alt_chunk_dim <- unlist(metadata$chunks)
  
  for (j in seq_len(ncol(required_chunks))) {
    index_in_result[[j]] <- which(chunk_idx[[j]] == required_chunks[i, j])
    ## are we requesting values outside the array due to overhanging chunks?
    outside_extent <- index_in_result[[j]] > metadata$shape[[j]]
    if (any(outside_extent))
      index_in_result[[j]] <- index_in_result[[j]][-outside_extent]
    if (any(index_in_result[[j]] == metadata$shape[[j]])) 
      alt_chunk_dim[j] <- length(index_in_result[[j]])
    
    index_in_chunk[[j]] <- ((index[[j]][index_in_result[[j]]] - 1) %% metadata$chunks[[j]]) + 1
  }
  
  ## read this chunk
  #chunk <- Rarr:::read_chunk(
  chunk <- my_read_chunk(
    zarr_array_path,
    chunk_id = required_chunks[i, ],
    metadata = metadata,
    s3_client = s3_client,
    alt_chunk_dim = alt_chunk_dim
  )

  warn <- chunk$warning[1]
  chunk_data <- chunk$chunk_data
  
  ## extract the required elements from the chunk
  selection <- R.utils::extract(chunk_data, indices = index_in_chunk, drop = FALSE)
  
  return(list(selection, index_in_result, warning = warn))
}

#change to FUN = .extract_elements
my_read_data <- function(required_chunks, zarr_array_path, s3_client, 
                      index, metadata) {
  warn <- 0L
  
  ## determine which chunk each of the requests indices belongs to
  chunk_idx <- mapply(\(x,y) { (x-1) %/% y }, index, metadata$chunks, SIMPLIFY = FALSE)
  
  ## hopefully we can eventually do this in parallel
  chunk_selections <- lapply(seq_len(nrow(required_chunks)), 
                             #FUN = .extract_elements,
                             FUN = my_extract_elements,
                             metadata = metadata, index = index,
                             required_chunks = required_chunks,
                             zarr_array_path = zarr_array_path,
                             s3_client = s3_client,
                             chunk_idx = chunk_idx)
  
  ## predefine our array to be populated from the read chunks
  output <- array(metadata$fill_value, dim = vapply(index, length, integer(1)))

  ## proceed in serial and update the output with each chunk selection in turn
  for (i in seq_along(chunk_selections)) {
    index_in_result <- chunk_selections[[i]][[2]]
    cmd <- Rarr:::.create_replace_call(x_name = "output", idx_name = "index_in_result",
                                idx_length = length(index_in_result), 
                                y_name = "chunk_selections[[i]][[1]]")
    eval(parse(text = cmd))
    warn <- max(warn, chunk_selections[[i]]$warning[1])
  }
  return(list(output = output, warn = warn))
}

my_read_zarr_array <- function (zarr_array_path, index, s3_client) 
{
    zarr_array_path <- Rarr:::.normalize_array_path(zarr_array_path)
    if (missing(s3_client)) 
        s3_client <- Rarr:::.create_s3_client(path = zarr_array_path)
    metadata <- Rarr:::read_array_metadata(zarr_array_path, s3_client = s3_client)
    #yikes
    metadata$datatype$base_type <- "float"
    
    if (missing(index)) {
        index <- vector(mode = "list", length = length(metadata$shape))
    }
    index <- Rarr:::check_index(index = index, metadata = metadata)
    required_chunks <- as.matrix(Rarr:::find_chunks_needed(metadata, index))
    res <- my_read_data(required_chunks, zarr_array_path, s3_client, index, metadata)
    if (isTRUE(res$warn > 0)) {
        warning("Integer overflow detected in at least one chunk.\n", 
            "Overflowing values have been replaced with NA", 
            call. = FALSE)
    }
    return(res$output)
}

#res <- Rarr:::read_data(required_chunks, zarr_array_path, s3_client, index, metadata)
#my_read_data(required_chunks, zarr_array_path, s3_client, index, metadata)$output

#Now runs...to 56K tibble
feat_id <- tibble(
  # #v2.1: comid = read_zarr_array("https://noaa-nwm-retrospective-2-1-zarr-pds.s3.amazonaws.com/chrtout.zarr/feature_id/")
  comid = my_read_zarr_array("https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/feature_id")
) |> 
  tibble::rowid_to_column(var = "i") |> 
  inner_join(sf_nhdp_wa_comid, by = "comid")

# {
#     "_ARRAY_DIMENSIONS": [
#         "time",
#         "feature_id"
#     ],
#     "add_offset": 0.0,
#     "coordinates": "latitude longitude",
#     "grid_mapping": "crs",
#     "long_name": "River Flow",
#     "missing_value": -999900,
#     "scale_factor": 0.009999999776482582,
#     "units": "m3 s-1"
# }
# Shape: 385704 x 2776734
# Chunk Shape: 672 x 30000
# No. of Chunks: 53382 (574 x 93)
# Data Type: int32

# q <- my_read_zarr_array(
#   "https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/streamflow/"
#   ,
#   index = list(1, feat_id$i)
# )


#for now hard coding expectation of passing year, month and hour(s)
#could add logic for other NULL handling to build time_dim_ind more flexibly
#defaults for single hour per day through full month (13:00 Aug 2020)
#may be faster to pull, for example, Apr-Sep rather than looping single months
#but then have [too] big objects and have to load and subset?
dir_data_common <- "~/T/DFW-Team WDFW Watershed Synthesis - data_common"

pull_nwm_zarr <- function(
    #s3_address = "https://noaa-nwm-retrospective-2-1-zarr-pds.s3.amazonaws.com/chrtout.zarr/streamflow/",
    s3_address =   "https://noaa-nwm-retrospective-3-0-pds.s3.amazonaws.com/CONUS/zarr/chrtout.zarr/streamflow/",
    y = 2022,
    m = 8,
    h = 13,
    feat = feat_id,
    dir_write = file.path(dir_data_common, "nwm_retro/v3_0")
) {
  sclfctr <- 0.009999999776482582
  cms2cfs <- 35.31468492
  
  #vector of all time available, v2.1 length was 367439
  time_dim <- seq(as.POSIXct("1979-02-01 01:00:00", tz = 'UTC'), length.out = 385704, by = "1 hour")
  #indices of desired subset
  time_i <- which(
    #year(time_dim) %in% 2023 & month(time_dim) %in% 8 & hour(time_dim) %in% 13
    year(time_dim) %in% y &
      month(time_dim) %in% m &
      hour(time_dim) %in% h
    )
  
  print(Sys.time())
  z <- my_read_zarr_array(s3_address, index = list(time_i, feat$i)) |> 
    as.data.frame() |> 
    set_names(feat$comid) |> 
    as_tibble() |> 
    mutate(
      #correct 'scale factor' brings to cms, then to cfs
      across(everything(), ~.* sclfctr * cms2cfs),
      across(everything(), ~if_else(.<0, NA_real_, .)),
      i = time_dim[time_i], #col of POSIXct
      m = month(i),
      yday = yday(i)
    )
  print(Sys.time())
  
  if(!is.null(dir_write)){
    if(identical(m, 1:12)) {
      f = file.path(dir_write, paste0("nwm_wa_",y, ".rds"))
    } else {
      f = file.path(dir_write, paste0("nwm_wa_",y,"_",paste0(str_pad(m,width = 2, pad = "0"),collapse = ""), ".rds"))
    }
    print(paste("data written to ", f))
    saveRDS(z, f)
  } else {
    return(z)
  }
}

# #365 days at 13:00 
# q <- pull_nwm_zarr(dir_write = NULL, y = 2022, m = 12)
# dim(as.matrix(q)); tail(names(q))
# q2 <- q |> select(-c(i:yday)) |> as.matrix()
# dim(q2)
# apply(q2,1, max, na.rm = T) #impossibly high values...>200k cfs
# rowMeans(q2)
# apply(q2,1, \(x) sum(is.na(x)))
# apply(q2,1, \(x) sum(x<0, na.rm = T)) #fixed, could drop these comids?
# #colnames(q2)[q2[1,]<0]
# colnames(q2)[q2[1,]<0]
# q2 <- q2[,!(is.na(q2[1,]))]
# rowMeans(q2)
# apply(q2,1, min)
# #okay looks like just a few weird/bad spots but the rescaling is correct
# plot.ecdf(q2[1,]) ; abline(v=c(10000,50000), col = 2:3)

# #default single hour (13:00) per day
# about 1m30s per y-m
# some change somewhere seems to have made less memory stable? 
# throws stubborn mem limit error after about 10 y-m
# gc() and googling did not show a better quick fix than just restarting session
# could script a meta-loop that spawns & provisions new session(s)
# either with Rscript or callr::r_bg, https://callr.r-lib.org/ 
# but this feels provisional and easy enough to babysit while doing other things
x <- expand_grid(y = 2021:2000, m = 1:12)
(x <- x |> filter(!(y == 2021 & m < 12)))
# for(i in seq_len(nrow(x))){
#   print(x[i,"y"]); print(x[i,"m"])
#   pull_nwm_zarr(y = x[i,"y"], m = x[i,"m"])
# }

#got annoyed and wrote something that starts fresh each y-m and seems more memory stable?
file.exists("~/T/DFW-Team WDFW Watershed Synthesis - General/hydrography/testing/nwm3_hacky_ym.R")

for(i in seq_len(nrow(x))){
  system2(
    command = "Rscript",
    args = c(
      #tilde expansion does NOT work
      shQuote("/Users/auerbdaa/T/DFW-Team WDFW Watershed Synthesis - General/hydrography/testing/nwm3_hacky_ym.R"),
      x[i,"y"], x[i,"m"]
    )
  )
  }

```

```{r nwm_on_3dhp}

#NWM 2022 testing values
q_3dhp <- q |> 
  select(m, yday, which(colnames(q) %in% as.character(unique(lines_3dhp$comid)))) |> 
  mutate(across(-c(m, yday), ~if_else(.<0 , NA_real_, .)))

q_3dhp_daily_summary <- q_3dhp |> 
  pivot_longer(cols = -c(m, yday), names_to = "comid", values_to = "cfs") |> 
  summarise(
    across(
      cfs, list(
        min = ~min(.), med = ~median(.), max = ~max(.),
        cv = ~sd(.) / mean(.)
        )
      ),
    .by = "comid"
    )

#add back to spatial objects
sf_nhdp_wa_aoi <- sf_nhdp_wa_aoi |> left_join(q_3dhp_daily_summary, by = "comid")
lines_3dhp <- lines_3dhp |> left_join(q_3dhp_daily_summary, by = "comid")

map(
  names(q_3dhp_daily_summary)[-1]
  ,
  ~sf_nhdp_wa_aoi |> 
    drop_na(cfs_min) |> 
    ggplot() + geom_sf(aes(color = eval(as.name(.x)), fill = eval(as.name(.x)))) +
    wacolors::scale_color_wa_c("diablo", reverse = T, aesthetics = c("color","fill")) +
    guides(color = guide_colorbar(.x), fill = guide_none())
  ) |> 
  wrap_plots(ncol = 2) +
  plot_annotation(subtitle = "NWM 3.0 2022 per med res NHDplus catchments")


lines_3dhp |> 
  ggplot() +
  geom_sf(aes(color = cfs_cv)) + 
  wacolors::scale_color_wa_c("stuart", midpoint = 1)


```

